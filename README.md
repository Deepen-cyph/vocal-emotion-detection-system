# vocal-emotion-detection-system
Introduction

This project aims to develop a machine learning model capable of detecting emotions from speech, addressing the growing demand for personalized experiences. By analyzing vocal inputs, our emotional speech recognition system can identify the speaker's emotions, enabling tailored recommendations and responses.

Applications

Marketing: Emotion-based product recommendations.
Automotive: Adjusting autonomous vehicle behavior based on driver emotions.
Healthcare: Monitoring patient emotions for better mental health support.
Customer Service: Enhancing interactions by adapting to customer emotions.
Entertainment: Personalized content and adaptive gaming experiences.

Dataset

RAVDESS:

Contains approximately 1,500 audio files from 24 actors (12 male, 12 female).
Each actor records short audios in 8 different emotions: neutral, calm, happy, sad, angry, fearful, disgust, and surprised.
The 7th character in each audio file's name indicates the emotion.
SAVEE:

Comprises around 500 audio files recorded by 4 different male actors.
The first two characters of the file name correspond to the different emotions portrayed.
